# 協調行動のシミュレーション

## 概要

以下の論文で提案された間接互恵行動のシミュレーションを実装。ただしmutationは実装していない。

Martin A. Nowak and Karl Sigmund, "Evolution of indirect reciprocity by image scoring", Nature vol. 393, pp. 573–577 (1998)

## アルゴリズム

* n人のプレイヤーを用意
* 各プレイヤーは、戦略値`k`とイメージスコア`score`、適合度`fitness`を持つ
* 各世代において、m回のDonorとRecipientが選ばれる
* DonorはRecipientのイメージスコア`score`を見て、自分の戦略値`k`よりも高ければ協力する、そうでなければ協力しない
    * Donorが協力を選んだ場合
        * Donorはコスト`c`を支払い、Recipientは報酬`b`を得る(それぞれの適合度が、`-c`、`+b`される)
        * Donorのイメージスコアが+1される(最大5)
    * Donorが非協力を選んだ場合
        * Donorのイメージスコアが-1される(最小-5)
        * それぞれの適合度は変化なし
* m回の相互作用の後、各プレイヤーは適合度に比例して子供を残す
* 子供は親から戦略値`k`は引き継ぐが、イメージスコアと適合度は引き継がない(クリアされる)

デフォルトのパラメータは以下の通り

* `n` プレイヤー数。デフォルトで100。
* `m` 世代ごとに何回相互作用させるか。デフォルトで125。
* `b` Donorが協力行動をとった場合にRecipientが受け取る報酬(適合度の増加分)。デフォルトで1.0。
* `c` Donorが協力行動をとった場合にDonorが支払うコスト(適合度の減少分)。デフォルトで0.1。

## 実行方法

実行すると、200世代後の戦略値`k`の人口を表示する。

以下の例では、戦略値`k=-5`の人口が37人、`k=5`の人口が53名の例。

```sh
$ python3 simulation.py
-5 37
-4 0
-3 0
-2 0
-1 0
0 0
1 0
2 0
3 0
4 0
5 63
6 0
```

実行するたびに結果が変わるが、多くの場合、一つの戦略値に収束する。どの戦略値が生き残るかはランダムだが、世代ごとの相互作用数`m`が大きいほど、協力的なプレイヤーが生き残りやすくなるため、小さな`k`に収束しやすくなる。例えば`m=1000`とすると、ほぼ`k<0`の戦略が生き残るようになる。

## 子孫の残し方の解説

各世代ごとにプレイヤーが稼いだ適合度`fitness`に比例して子孫を残すが、子孫に引き継ぐのは戦略値`k`のみであるから、まず各戦略値`k`ごとに適合度を合計し、新世代の戦略値は`k`「旧世代の各戦略値`k`が稼いだ適合度」に比例して選べば良い。

```py
fitness_list = {}  # 戦略値kごとのfitnessの合計
for i in range(-5, 7):
    fitness_list[i] = 0

for p in players:
    fitness_list[p.k] += p.fitness
```

ここまでで、`fitness_list`という辞書に、各戦略値ごとの適合度の合計が入る。そこから、各戦略値のリストと、適合度のリストを作成する。

```py
k_list = list(fitness_list.keys())
f_list = list(fitness_list.values())
```

Pythonには重みに比例してランダムいインデックスを選ぶ関数、`random.choices`が存在する。この「選ぶインデックス」として`k_list`を、「インデックスを選ぶ重み」として`f_list`を渡せば、`f_list`の大きさに比例して`k_list`から値をランダムに選んでくれる。

```py
p.k = random.choices(k_list, weights=f_list)[0]
```

ただし、`random.choices`はリストを返すので、例えば`1`が欲しいのに`[1]`として渡されてくる。そこで、その0番目のインデックスを受け取るために`[0]`を最後につけている。

## LICENSE

MIT
